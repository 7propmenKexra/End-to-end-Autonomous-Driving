# End-to-end-Autonomous-Driving
Frontiers and Challenges for End-to-end Autonomous Driving

![](https://img.shields.io/badge/Record-84-673ab7.svg)
![](https://img.shields.io/badge/License-MIT-lightgrey.svg)

> background

## Table of Contents

## At-a-Glance

![](figs/overview.jpg)

TODO

## Paper Collection

xxx categorized in topics.

### Survey
- A Survey of End-to-End Driving: Architectures and Training Methods [[TNNLS2020]](https://arxiv.org/abs/2003.06404)
- A Survey of Deep RL and IL for Autonomous Driving Policy Learning [[TITS2021]](https://arxiv.org/abs/2101.01993)
- A Survey on Imitation Learning Techniques for End-to-End Autonomous Vehicles [[TITS2022]](https://arxiv.org/abs/2101.01993)
- 

### Multi-modal Fusion
- Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving [[CVPR2023]](https://arxiv.org/abs/2305.06242)[[code]](https://github.com/OpenDriveLab/ThinkTwice)![](https://img.shields.io/github/stars/OpenDriveLab/ThinkTwice.svg?style=social&label=Star&maxAge=2592000)

- ReasonNet: End-to-End Driving with Temporal and Global Reasoning [[CVPR2023]](https://arxiv.org/abs/2305.10507)

- Safety-enhanced autonomous driving using interpretable sensor fusion transformer [[CoRL2022]](https://arxiv.org/abs/2207.14024) [[Code]](https://github.com/opendilab/InterFuser)![](https://img.shields.io/github/stars/opendilab/InterFuser.svg?style=social&label=Star&maxAge=2592000)

- Multi-modal fusion transformer for end-to-end autonomous driving [[CVPR2021]](https://arxiv.org/abs/2104.09224)[[Code]](https://github.com/autonomousvision/transfuser.git)![](https://img.shields.io/github/stars/autonomousvision/transfuser.svg?style=social&label=Star&maxAge=2592000)
    
- Transfuser: Imitation with transformer-based sensor fusion for autonomous driving [[TPAMI2022]](https://arxiv.org/abs/2205.15997)[[Code]](https://github.com/autonomousvision/transfuser.git)![](https://img.shields.io/github/stars/autonomousvision/transfuser.svg?style=social&label=Star&maxAge=2592000)
    
- Probabilistic end-to-end vehicle navigation in complex dynamic environments with multimodal sensor fusion [[RAL2020]](https://arxiv.org/abs/2005.01935)

- Learning from all vehicles [[CVPR2022]](http://arxiv.org/pdf/1709.04622v4)[[Code]](https://github.com/dotchen/LAV.git)![](https://img.shields.io/github/stars/dotchen/LAV.svg?style=social&label=Star&maxAge=2592000)
    
- Urban driving with conditional imitation learning [[ICRA2020]](http://arxiv.org/pdf/1912.00177v2)
    
- Multi-modal sensor fusion-based deep neural network for end-to-end autonomous driving with scene understanding(IEEESJ, 2020)

- Intentnet: Learning to predict intention from raw sensor data [[CoRL2018]](https://arxiv.org/abs/2101.07907)
    
- Coopernaut: End-to-end driving with cooperative perception for networked vehicles [[CVPR2022]](https://arxiv.org/abs/2205.02222)[[Code]](https://github.com/UT-Austin-RPL/Coopernaut.git)![](https://img.shields.io/github/stars/UT-Austin-RPL/Coopernaut.svg?style=social&label=Star&maxAge=2592000)
    
- Deep federated learning for autonomous driving(IV, 2022)[[Paper]](http://arxiv.org/pdf/2110.05754v2)[[Code]](https://github.com/aioz-ai/FADNet.git)![](https://img.shields.io/github/stars/aioz-ai/FADNet.svg?style=social&label=Star&maxAge=2592000)
    
- Deductive reinforcement learning for visual autonomous urban driving navigation(TNNLS, 2021)
- Using eye gaze to enhance generalization of imitation networks to unseen environments(TNNLS,2020)
- Toward deep reinforcement learning without a simulator: An autonomous steering example(AAAI, 2018)
- Agile autonomous driving using end-to-end deep imitation learning(RSSXIV, 2017)[[Paper]](https://arxiv.org/abs/1709.07174)
- Multimodal End-to-End Autonomous Driving(TITS, 2020)
- Does computer vision matter for action?(Science Robotics, 2019)
- End-to-end interpretable neural motion planner(CVPR, 2019)
- Carl-Lead: Lidar-based End-to-End Autonomous Driving with Contrastive Deep Reinforcement Learning(arxiv, 2021)
- SEM2: Enhance sample efficiency and robustness of end-to-end urban autonomous driving via semantic masked world model(NeurIPS Workshop 2022)
- Interpretable endto-end urban autonomous driving with latent deep reinforcement learning(TITS, 2022)
- Multi-modal sensor fusion-based deep neural network for end-to-end autonomous driving with scene understanding(Sensors, 2020)
- Fully end-to-end autonomous driving with semantic depth cloud mapping and multiagent(IV, 2022)
- End-to-end multi-modal sensors fusion system for urban automated driving(NeurIPS Workshop, 2018)
- Lidar-video driving dataset: Learning driving policies effectively(CVPR 2018)
- Multinet: Multimodal multi-task learning for autonomous driving(WACV, 2019)
- Mmfn: Multi-modal-fusion-net for end-to-end driving(IROS, 2022)
- Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving(CVPR, 2023)

    
### Multi-task Learning
- Transfuser: Imitation with transformer-based sensor fusion for autonomous driving(PAMI, 2022)[[Paper]](https://arxiv.org/abs/2205.15997)[[Code]](https://github.com/autonomousvision/transfuser.git)![](https://img.shields.io/github/stars/autonomousvision/transfuser.svg?style=social&label=Star&maxAge=2592000)
    
- Safety-enhanced autonomous driving using interpretable sensor fusion transformer(CoRL, 2022)[[Paper]](https://arxiv.org/abs/2207.14024)[[Code]](https://github.com/opendilab/InterFuser.git)![](https://img.shields.io/github/stars/opendilab/InterFuser.svg?style=social&label=Star&maxAge=2592000)
    
- Trajectoryguided control prediction for end-to-end autonomous driving: A simple yet strong baseline(NeurIPS, 2022)
- Learning from all vehicles(CVPR, 2022)[[Paper]](http://arxiv.org/pdf/1709.04622v4)[[Code]](https://github.com/dotchen/LAV.git)![](https://img.shields.io/github/stars/dotchen/LAV.svg?style=social&label=Star&maxAge=2592000)
    
- Neat: Neural attention fields for end-to-end autonomous driving(ICCV, 2021)[[Paper]](https://arxiv.org/abs/2109.04456)[[Code]](https://github.com/autonomousvision/neat.git)![](https://img.shields.io/github/stars/autonomousvision/neat.svg?style=social&label=Star&maxAge=2592000)
    
- Sam: Squeeze-and-mimic networks for conditional visual driving policy learning(CoRL, 2020)[[Paper]](https://arxiv.org/abs/1912.02973)[[Code]](https://github.com/twsq/sam-driving.git)![](https://img.shields.io/github/stars/twsq/sam-driving.svg?style=social&label=Star&maxAge=2592000)
    
- Multitask learning with attention for end-to-end autonomous driving(CVPRW, 2021)
- Urban driving with conditional imitation learning(ICRA, 2020)[[Paper]](http://arxiv.org/pdf/1912.00177v2)
   
- Multi-modal sensor fusionbased deep neural network for end-to-end autonomous driving with scene understanding(IEEE Sensors Journal, 2020)
- Multi-task learning with future states for vision-based autonomous driving(2020)
- Learning to steer by mimicking features from heterogeneous auxiliary networks(AAAI, 2019)[[Paper]](http://arxiv.org/pdf/1811.02759v1)[[Code]](https://github.com/cardwing/Codes-for-Steering-Control.git)![](https://img.shields.io/github/stars/cardwing/Codes-for-Steering-Control.svg?style=social&label=Star&maxAge=2592000)
    
- Multinet: Multi-modal multi-task learning for autonomous driving(WACV,2019)[[Paper]](https://arxiv.org/abs/1709.05581)
    
- Intentnet: Learning to predict intention from raw sensor data(CoRL, 2018)[[Paper]](https://arxiv.org/abs/2101.07907)
    
- Rethinking self-driving: Multi-task knowledge for better generalization and accident explanation ability(arXiv, 2018)[[Paper]](https://arxiv.org/abs/1809.11100)[[Code]](https://github.com/jackspp/rethinking-self-driving.git)![](https://img.shields.io/github/stars/jackspp/rethinking-self-driving.svg?style=social&label=Star&maxAge=2592000)
   
- Learning end-to-end autonomous driving using guided auxiliary supervision(ICVGIP, 2018)
- End-to-end learning of driving models from large-scale video datasets(CVPR, 2017)[[Paper]](https://arxiv.org/abs/1612.01079)[[Code]](https://github.com/gy20073/BDD_Driving_Model.git)![](https://img.shields.io/github/stars/gy20073/BDD_Driving_Model.svg?style=social&label=Star&maxAge=2592000)
    

### BEV
- Transfuser: Imitation with transformer-based sensor fusion for autonomous driving(PAMI, 2022)[[Paper]](https://arxiv.org/abs/2205.15997)[[Code]](https://github.com/autonomousvision/transfuser.git)![](https://img.shields.io/github/stars/autonomousvision/transfuser.svg?style=social&label=Star&maxAge=2592000)
    
- Driving through ghosts: Behavioral cloning with false positives(IROS, 2020)[[Paper]](https://arxiv.org/abs/2008.12969)
    
- Learning from all vehicles(CVPR, 2022)[[Paper]](http://arxiv.org/pdf/1709.04622v4)[[Code]](https://github.com/dotchen/LAV.git)![](https://img.shields.io/github/stars/dotchen/LAV.svg?style=social&label=Star&maxAge=2592000)
   
- Neat: Neural attention fields for end-to-end autonomous driving(ICCV, 2021)[[Paper]](https://arxiv.org/abs/2109.04456)[[Code]](https://github.com/autonomousvision/neat.git)![](https://img.shields.io/github/stars/autonomousvision/neat.svg?style=social&label=Star&maxAge=2592000)
    
- Deep federated learning for autonomous driving(IV, 2022)[[Paper]](http://arxiv.org/pdf/2110.05754v2)[[Code]](https://github.com/aioz-ai/FADNet.git)![](https://img.shields.io/github/stars/aioz-ai/FADNet.svg?style=social&label=Star&maxAge=2592000)
    
- Observenet control: A vision-dynamics learning approach to predictive control in autonomous vehicles(RAL, 2021)[[Paper]](https://arxiv.org/abs/2107.08690)
    
- Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d(ECCV, 2020)[[Paper]](https://arxiv.org/abs/2008.05711)[[Code]](https://github.com/nv-tlabs/lift-splat-shoot.git)![](https://img.shields.io/github/stars/nv-tlabs/lift-splat-shoot.svg?style=social&label=Star&maxAge=2592000)

- ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning(ECCV, 2022)

- Planning-oriented Autonomous Driving(CVPR, 2023)

- Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving(CVPR, 2023)


### Interpretability
- Grounding human-to-vehicle advice for self-driving vehicles(CVPR, 2019)[[Paper]](https://arxiv.org/abs/1911.06978)
    
- Safety-enhanced autonomous driving using interpretable sensor fusion transformer(CoRL, 2022)[[Paper]](https://arxiv.org/abs/2207.14024)[[Code]](https://github.com/opendilab/InterFuser.git)![](https://img.shields.io/github/stars/opendilab/InterFuser.svg?style=social&label=Star&maxAge=2592000)
   
- Model-based imitation learning for urban driving(NeurIPS, 2022)[[Paper]](https://arxiv.org/abs/2210.07729)[[Code]](https://github.com/wayveai/mile.git)![](https://img.shields.io/github/stars/wayveai/mile.svg?style=social&label=Star&maxAge=2592000)
    
- Approximate inverse reinforcement learning from vision-based imitation learning(ICRA, 2021)[[Paper]](https://arxiv.org/pdf/2004.08051.pdf)
    
- Exploring the limitations of behavior cloning for autonomous driving(2019)[[Paper]](https://arxiv.org/abs/1904.08980)[[Code]](https://github.com/felipecode/coiltraine.git)![](https://img.shields.io/github/stars/felipecode/coiltraine.svg?style=social&label=Star&maxAge=2592000)
    
- Deep steering: Learning end-to-end driving model from spatial and temporal visual cues(arXiv, 2017)[[Paper]](https://arxiv.org/abs/1708.03798)[[Code]](https://github.com/abhileshborode/Behavorial-Clonng-Self-driving-cars.git)![](https://img.shields.io/github/stars/abhileshborode/Behavorial-Clonng-Self-driving-cars.svg?style=social&label=Star&maxAge=2592000)
- End-to-end interpretable neural motion planner(CVPR, 2019)
- Dsdnet: Deep structured self-driving network(ECCV 2020)
- Perceive, predict, and plan: Safe motion planning through interpretable semantic representations(ECCV, 2020)
- Mp3: A unified model to map, perceive, predict and plan(CVPR, 2021)
- Safe local motion planning with self-supervised freespace forecasting(CVPR, 2021)
- Differentiable raycasting for self-supervised occupancy forecasting(ECCV, 2022)
- Explainability of deep vision-based autonomous driving systems: Review and challenges(IJCV, 2022)
- Explaining how a deep neural network trained with end-to-end learning steers a car(arxiv, 2017)
- Visualbackprop: Efficient visualization of cnns for autonomous driving(ICRA, 2018)
- Predicting model failure using saliency maps in autonomous driving systems(arxiv, 2019)
- Interpretable learning for self-driving cars by visualizing causal attention(ICCV, 2017)
- Multi-task learning with attention for end-to-end autonomous driving(CVPRW, 2021)
- Visual explanation by attention branch network for end-to-end learning-based self-driving(IV, 2019)
- Deep object-centric policies for autonomous driving(ICRA, 2019)
- Explaining autonomous driving by learning end-to-end visual attention(CVPRW, 2020)
- Scaling self-supervised end-to-end driving with multiview attention learning(arxiv, 2023)
- Plant: Explainable planning transformers via object-level representations(CoRL, 2022)
- Textual explanations for self-driving vehicles(ICCV, 2018)
- Driving behavior explanation with multi-level fusion(Pattern Recognition, 2022)
- Explainable object-induced action decision for autonomous vehicles(CVPR, 2020)
- Adapt: Action-aware driving caption transformer(ICRA, 2023)


### Causal Confusion
- Offroad obstacle avoidance through end-to-end learning(NeurIPS, 2005)
- Shortcut learning in deep neural networks(Nature Machine Intelligence, 2020)
- Causal confusion in imitation learning(NeurIPS, 2019)
- Fighting fire with fire: avoiding dnn shortcuts through priming(ICML, 2022)[[Paper]](https://arxiv.org/abs/2206.10816)
- Keyframefocused visual imitation learning(ICML, 2021)
- Fighting copycat agents in behavioral cloning from observation histories(NeurIPS, 2020)[[Paper]](http://arxiv.org/pdf/2010.14876v1)
- Resolving Copycat Problems in Visual Imitation Learning via Residual Action Prediction(ECCV, 2022)
- Object-aware regularization for addressing causal confusion in imitation learning(NeurIPS, 2021)[[Paper]](https://arxiv.org/abs/2110.14118)[[Code]](https://github.com/alinlab/oreo.git)![](https://img.shields.io/github/stars/alinlab/oreo.svg?style=social&label=Star&maxAge=2592000)
- Chauffeurnet:Learning to drive by imitating the best and synthesizing the worst(RSS, 2019)
- Exploring the limitations of behavior cloning for autonomous driving(ICCV, 2019)

### Transformer
- Multi-modal fusion transformer for end-to-end autonomous driving(CVPR, 2021)[[Paper]](https://arxiv.org/abs/2104.09224)[[Code]](https://github.com/autonomousvision/transfuser.git)![](https://img.shields.io/github/stars/autonomousvision/transfuser.svg?style=social&label=Star&maxAge=2592000)
    
- Transfuser: Imitation with transformer-based sensor fusion for autonomous driving(PAMI, 2022)[[Paper]](https://arxiv.org/abs/2205.15997)[[Code]](https://github.com/autonomousvision/transfuser.git)![](https://img.shields.io/github/stars/autonomousvision/transfuser.svg?style=social&label=Star&maxAge=2592000)
    
- Ground then navigate: Language-guided navigation in dynamic scenes(arXiv, 2022)[[Paper]](https://arxiv.org/abs/2209.11972)
    
- Safety-enhanced autonomous driving using interpretable sensor fusion transformer(CoRL, 2022)[[Paper]](https://arxiv.org/abs/2207.14024)[[Code]](https://github.com/opendilab/InterFuser.git)![](https://img.shields.io/github/stars/opendilab/InterFuser.svg?style=social&label=Star&maxAge=2592000)
    
- Neat: Neural attention fields for end-to-end autonomous driving(ICCV, 2021)[[Paper]](https://arxiv.org/abs/2109.04456)[[Code]](https://github.com/autonomousvision/neat.git)![](https://img.shields.io/github/stars/autonomousvision/neat.svg?style=social&label=Star&maxAge=2592000)
    
- Model-based imitation learning for urban driving(NeurIPS, 2022)[[Paper]](https://arxiv.org/abs/2210.07729)[[Code]](https://github.com/wayveai/mile.git)![](https://img.shields.io/github/stars/wayveai/mile.svg?style=social&label=Star&maxAge=2592000)
    
- Coopernaut: End-to-end driving with cooperative perception for networked vehicles(CVPR, 2022)[[Paper]](https://arxiv.org/abs/2205.02222)[[Code]](https://github.com/UT-Austin-RPL/Coopernaut.git)![](https://img.shields.io/github/stars/UT-Austin-RPL/Coopernaut.svg?style=social&label=Star&maxAge=2592000)
    
- Cadre: A cascade deep reinforcement learning framework for vision-based autonomous urban driving(AAAI, 2022)[[Paper]](https://arxiv.org/abs/2202.08557)[[Code]](https://github.com/BIT-MCS/Cadre.git)![](https://img.shields.io/github/stars/BIT-MCS/Cadre.svg?style=social&label=Star&maxAge=2592000)
    
- Human-AI shared control via policy dissection(NeurIPS, 2022)[[Paper]](https://arxiv.org/abs/2206.00152)[[Code]](https://github.com/Mehooz/vision4leg.git)

- Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving(CVPR, 2023)


### Data Augmentation, adversarial training, DAgger
- King: Generating safety-critical driving scenarios for robust imitation via kinematics gradients(ECCV, 2022)[[Paper]](https://arxiv.org/abs/2204.13683)[[Code]](https://github.com/autonomousvision/transfuser.git)![](https://img.shields.io/github/stars/autonomousvision/transfuser.svg?style=social&label=Star&maxAge=2592000)
    
- Learning from all vehicles(CVPR, 2022)[[Paper]](http://arxiv.org/pdf/1709.04622v4)[[Code]](https://github.com/dotchen/LAV.git)![](https://img.shields.io/github/stars/dotchen/LAV.svg?style=social&label=Star&maxAge=2592000)
    
- Learning by cheating(CoRL, 2020)[[Paper]](http://arxiv.org/pdf/2107.00123v1)[[Code]](https://github.com/dotchen/LearningByCheating.git)![](https://img.shields.io/github/stars/dotchen/LearningByCheating.svg?style=social&label=Star&maxAge=2592000)
    
- Driving among flatmobiles: Bird-eye-view occupancy grids from a monocular camera for holistic trajectory planning(WACV, 2021)[[Paper]](https://arxiv.org/abs/2008.04047)
    
- Learning situational driving(2020)[[Paper]](http://arxiv.org/pdf/1811.07868v2)
    
- Learning accurate, comfortable and human-like driving(arXiv, 2019)[[Paper]](https://arxiv.org/abs/1903.10995)
    
- Query-efficient imitation learning for endto-end simulated driving(AAAI, 2017)
- Meta learning framework for automated driving(arXiv, 2017)[[Paper]](http://arxiv.org/pdf/1706.04038v1)
    
- Explaining how a deep neural network trained with end-to-end learning steers a car(arXiv, 2017)[[Paper]](https://arxiv.org/abs/1704.07911)[[Code]](https://github.com/ermolenkodev/keras-salient-object-visualisation.git)![](https://img.shields.io/github/stars/ermolenkodev/keras-salient-object-visualisation.svg?style=social&label=Star&maxAge=2592000)
    
- Gradient-free adversarial training against image corruption for learning-based steering(NeurIPS, 2021)
- Adversarial driving: Attacking end-to-end autonomous driving systems(arXiv, 2021)[[Paper]](https://arxiv.org/abs/2103.09151)[[Code]](https://github.com/wuhanstudio/adversarial-driving.git)![](https://img.shields.io/github/stars/wuhanstudio/adversarial-driving.svg?style=social&label=Star&maxAge=2592000)
    
- Improving the generalization of end-to-end driving through procedural generation(arXiv, 2020)[[Paper]](https://arxiv.org/abs/2012.13681)[[Code]](https://github.com/decisionforce/pgdrive.git)![](https://img.shields.io/github/stars/decisionforce/pgdrive.svg?style=social&label=Star&maxAge=2592000)
    
- Learning driving models from parallel end-to-end driving data set()
- Simple physical adversarial examples against end-to-end autonomous driving models(ICESS, 2019)[[Paper]](https://arxiv.org/abs/1903.05157)
    
- Isolating and leveraging controllable and noncontrollable visual dynamics in world models(NeurIPS, 2022)[[Paper]](http://arxiv.org/pdf/2205.13817v3)[[Code]](https://github.com/panmt/Iso-Dream.git)![](https://img.shields.io/github/stars/panmt/Iso-Dream.svg?style=social&label=Star&maxAge=2592000)
    
- Distributional reinforcement learning for efficient exploration(ICML, 2019)[[Paper]](http://arxiv.org/pdf/2102.05710v1)
    
- Enhanced transfer learning for autonomous driving with systematic accident simulation(IROS, 2020)
- Improving the generalization of end-to-end driving through procedural generation(arxiv, 2020)
- Microscopic traffic simulation using sumo(ITSC, 2018)
- Trafficsim: Learning to simulate realistic multi-agent behaviors(CVPR, 2021)
- Scalable end-to-end autonomous vehicle testing via rare-event simulation(NeurIPS, 2018)
- Generating adversarial driving scenarios in high-fidelity simulators(ICRA, 2019)
- Learning to collide: An adaptive safety-critical scenarios generating method(IROS, 2020)
- Multimodal Safety-Critical Scenarios Generation for Decision-Making Algorithms Evaluation(RA-L, 2021)
- Advsim: Generating safety-critical scenarios for self-driving vehicles(CVPR, 2021)
- A reduction of imitation learning and structured prediction to no-regret online learning(AISTATS, 2011)
- Agile autonomous driving using end-to-end deep imitation learning(RSS, 2017)
- Exploring data aggregation in policy learning for vision-based urban autonomous driving(CVPR, 2020)


### Language Guided
- Ground then navigate: Language-guided navigation in dynamic scenes(arXiv, 2022)[[Paper]](https://arxiv.org/abs/2209.11972)
    
- Conditional driving from natural language instructions(CoRL, 2019)[[Paper]](https://arxiv.org/abs/1910.07615)
    
- Grounding human-to-vehicle advice for self-driving vehicles(CVPR, 2019)[[Paper]](https://arxiv.org/abs/1911.06978)
    
- Talk to the vehicle: Language conditioned autonomous navigation of self driving cars(IROS, 2019)
- Advisable learning for self-driving vehicles by internalizing observation-toaction rules(CVPR, 2020)
- Talk2car: Taking control of your selfdriving car(EMNLP, 2019)
- Learning to navigate in cities without a map(NeurIPS, 2018)
- Touchdown: Natural language navigation and spatial reasoning in visual street environments(CVPR, 2019)
- Generating landmark navigation instructions from maps as a graph-to-text problem(ACL, 2021)
- Grounding human-to-vehicle advice for self-driving vehicles(CVPR, 2019)
- Talk to the vehicle:Language conditioned autonomous navigation of self driving cars(IROS, 2019)
- Advisable learning for self-driving vehicles by internalizing observation-to-action rules(CVPR, 2020)
- Conditional driving from natural language instructions(CoRL, 2019)
- Ground then navigate: Language-guided navigation in dynamic scenes(arxiv, 2022)
- Lm-nav:Robotic navigation with large pre-trained models of language, vision, and action(CoRL, 2023)
- Learning transferable visual models from natural language supervision(ICML, 2021)

### Sim2real
- Enhanced transfer learning for autonomous driving with systematic accident simulation(IROS, 2020)[[Paper]](https://arxiv.org/abs/2007.12148)
    
- Learning to drive from simulation without real world labels(ICRA, 2019)[[Paper]](https://arxiv.org/abs/1812.03823)
    
- Visual-based autonomous driving deployment from a stochastic and uncertainty-aware perspective(IROS, 2019)
- Virtual to real reinforcement learning for autonomous driving(BMVC, 2017)
- Domain adaptation in reinforcement learning via latent unified state representation(AAAI, 2021)
- Segmented encoding for sim2real of rl-based end-to-end autonomous driving(IV, 2022)
- Simulation-based reinforcement learning for real-world autonomous driving(ICRA, 2020)
- A versatile and efficient reinforcement learning framework for autonomous driving(arxiv, 2021)

### Uncertainty
- Visual-based autonomous driving deployment from a stochastic and uncertainty-aware perspective(IROS, 2019)
- Driving through ghosts: Behavioral cloning with false positives(IROS, 2020)[[Paper]](https://arxiv.org/abs/2008.12969)
    
- Probabilistic end-to-end vehicle navigation in complex dynamic environments with multimodal sensor fusion(RAL, 2020)[[Paper]](https://arxiv.org/abs/2005.01935)
    
- Vtgnet: A vision-based trajectory generation network for autonomous vehicles in urban environments(T-IV, 2020)[[Paper]](https://arxiv.org/abs/2004.12591)[[Code]](https://github.com/caipeide/VTGNet.git)![](https://img.shields.io/github/stars/caipeide/VTGNet.svg?style=social&label=Star&maxAge=2592000)
    
- Evaluating uncertainty quantification in end-to-end autonomous driving control(arXiv, 2018)[[Paper]](https://arxiv.org/abs/1811.06817)
    
- Can autonomous vehicles identify, recover from, and adapt to distribution shifts?(ICML, 2020)[[Paper]](https://arxiv.org/abs/2006.14911)[[Code]](https://github.com/OATML/oatomobile.git)![](https://img.shields.io/github/stars/OATML/oatomobile.svg?style=social&label=Star&maxAge=2592000)
    

### v2v cooperative
- Cadre: A cascade deep reinforcement learning framework for vision-based autonomous urban driving(AAAI, 2022)[[Paper]](https://arxiv.org/abs/2202.08557)[[Code]](https://github.com/BIT-MCS/Cadre.git)![](https://img.shields.io/github/stars/BIT-MCS/Cadre.svg?style=social&label=Star&maxAge=2592000)
    

### federated learning
- Interpretable end-to-end urban autonomous driving with latent deep reinforcement learning(T-ITS, 2022)[[Paper]](https://arxiv.org/abs/2001.08726)[[Code]](https://github.com/cjy1992/interp-e2e-driving.git)![](https://img.shields.io/github/stars/cjy1992/interp-e2e-driving.svg?style=social&label=Star&maxAge=2592000)
    
### Representation Learning, Affordance Learning
- Segmented encoding for sim2real of rl-based end-to-end autonomous driving(IV, 2022)
- Gri: General reinforced imitation and its application to vision-based autonomous driving(arXiv, 2021)[[Paper]](https://arxiv.org/abs/2111.08575)
    
- Policy-based reinforcement learning for training autonomous driving agents in urban areas with affordance learning(T-ITS, 2021)
- Latent attention augmentation for robust autonomous driving policies(IROS, 2021)
- Multi-task long-range urban driving based on hierarchical planning and reinforcement learning(ITSC, 2021)
- Carl-lead: Lidar-based end-to-end autonomous driving with contrastive deep reinforcement learning(arXiv, 2021)[[Paper]](https://arxiv.org/abs/2109.08473)
    
- End-to-end model-free reinforcement learning for urban driving using implicit affordances(CVPR, 2020)
- Learning to drive by watching youtube videos: Action-conditioned contrastive policy pretraining(ECCV, 2022)[[Paper]](https://arxiv.org/abs/2204.02393)
    
- Pretrained image encoder for generalizable visual reinforcement learning(NeurIPS, 2022)
- Task-induced representation learning(ICLR, 2022)[[Paper]](https://arxiv.org/abs/2204.11827)
    
- Learning generalizable representations for reinforcement learning via adaptive meta-learner of behavioral similarities(ICLR, 2022)[[Paper]](https://arxiv.org/abs/2212.13088)[[Code]](https://github.com/jianda-chen/AMBS.git)![](https://img.shields.io/github/stars/jianda-chen/AMBS.svg?style=social&label=Star&maxAge=2592000)
    
- Driver behavioral cloning for route following in autonomous vehicles using task knowledge distillation(T-IV,2022)
- Policy pre-training for autonomous driving via self-supervised geometric modeling(ICLR, 2023)

### Expert Demo
- Gri: General reinforced imitation and its application to vision-based autonomous driving(arXiv, 2021)[[Paper]](https://arxiv.org/abs/2111.08575)
    
- Fully end-to-end autonomous driving with semantic depth cloud mapping and multi-agent(IV, 2022)
- Differentiable control barrier functions for vision-based end-to-end autonomous driving(arXiv, 2022)[[Paper]](https://arxiv.org/abs/2203.02401)
    
- Generative planning for temporally coordinated exploration in reinforcement learning(ICLR, 2022)[[Paper]](http://arxiv.org/pdf/2201.09765v2)[[Code]](https://github.com/Haichao-Zhang/generative-planning.git)![](https://img.shields.io/github/stars/Haichao-Zhang/generative-planning.svg?style=social&label=Star&maxAge=2592000)
   

### World Model
- SEM2: Enhance sample efficiency and robustness of end-to-end urban autonomous driving via semantic masked world model(NeurIPS, 2022)
- Addressing optimism bias in sequence modeling for reinforcement learning(ICML, 2022)[[Paper]](https://arxiv.org/abs/2207.10295)
    
[//]: # (- Mmfn: Multi-modal-fusion-net for end-to-end driving&#40;IROS, 2022&#41;[[Paper]]&#40;https://arxiv.org/abs/2207.00186&#41;[[Code]]&#40;https://github.com/Kin-Zhang/mmfn.git&#41;![]&#40;https://img.shields.io/github/stars/Kin-Zhang/mmfn.svg?style=social&label=Star&maxAge=2592000&#41;)
    
[//]: # (- Learning mixture of domain-specific experts via disentangled factors for autonomous driving&#40;AAAI, 2022&#41;)
- An end-to-end curriculum learning approach for autonomous driving scenarios(T-ITS, 2022)

[//]: # (- Hierarchical program-triggered reinforcement learning agents for automated driving&#40;T-ITS, 2021&#41;[[Paper]]&#40;https://arxiv.org/abs/2103.13861&#41;)
    
[//]: # (- Behavior transformers: Cloning k modes with one stone&#40;NeurIPS, 2022&#41;[[Paper]]&#40;https://arxiv.org/abs/2206.11251&#41;[[Code]]&#40;https://github.com/notmahi/bet.git&#41;![]&#40;https://img.shields.io/github/stars/notmahi/bet.svg?style=social&label=Star&maxAge=2592000&#41;)
- Learning to drive from a world on rails(ICCV, 2021)
- Interpretable end-to-end urban autonomous driving with latent deep reinforcement learning(TITS, 2022)
- Uncertainty-aware modelbased reinforcement learning: Methodology and application in autonomous driving(IV, 2022)
- Isolating and leveraging controllable and noncontrollable visual dynamics in world models(NeurIPS, 2022)
- Deductive reinforcement learning for visual autonomous urban driving navigation(TNNLS, 2021)
- Model-based imitation learning for urban driving(NeurIPS, 2022)
- 

### BEV
- SEM2: Enhance sample efficiency and robustness of end-to-end urban autonomous driving via semantic masked world model(NeurIPS, 2022)
- Addressing optimism bias in sequence modeling for reinforcement learning(ICML, 2022)[[Paper]](https://arxiv.org/abs/2207.10295)
    
- Learning mixture of domain-specific experts via disentangled factors for autonomous driving(AAAI, 2022)

### Multi-modal Fusion
- Multi-task long-range urban driving based on hierarchical planning and reinforcement learning(ITSC, 2021)
- Carl-lead: Lidar-based end-to-end autonomous driving with contrastive deep reinforcement learning(arXiv, 2021)[[Paper]](https://arxiv.org/abs/2109.08473)
    
- Addressing optimism bias in sequence modeling for reinforcement learning(ICML, 2022)[[Paper]](http://arxiv.org/pdf/2207.10295v1)
    
- Learning mixture of domain-specific experts via disentangled factors for autonomous driving(AAAI, 2022)

### Transformer
- Mmfn: Multi-modal-fusion-net for end-to-end driving(IROS, 2022)[[Paper]](https://arxiv.org/abs/2207.00186)[[Code]](https://github.com/Kin-Zhang/mmfn.git)![](https://img.shields.io/github/stars/Kin-Zhang/mmfn.svg?style=social&label=Star&maxAge=2592000)
    
- Safe driving via expert guided policy optimization(CoRL, 2022)[[Paper]](http://arxiv.org/pdf/2110.06831v2)[[Code]](https://github.com/decisionforce/EGPO.git)![](https://img.shields.io/github/stars/decisionforce/EGPO.svg?style=social&label=Star&maxAge=2592000)
    
- Multi-modal fusion transformer for end-to-end autonomous driving(CVPR, 2021)[[Paper]](https://arxiv.org/abs/2104.09224)[[Code]](https://github.com/autonomousvision/transfuser.git)![](https://img.shields.io/github/stars/autonomousvision/transfuser.svg?style=social&label=Star&maxAge=2592000)
    
- Transfuser: Imitation with transformer-based sensor fusion for autonomous driving(PAMI,2022)[[Paper]](https://arxiv.org/abs/2205.15997)[[Code]](https://github.com/autonomousvision/transfuser.git)![](https://img.shields.io/github/stars/autonomousvision/transfuser.svg?style=social&label=Star&maxAge=2592000)
   

### Distributed
- Gri: General reinforced imitation and its application to vision-based autonomous driving(arXiv, 2021)[[Paper]](https://arxiv.org/abs/2111.08575)
    
- End-to-end model-free reinforcement learning for urban driving using implicit affordances(CVPR, 2020)
- Safe driving via expert guided policy optimization(CoRL, 2022)[[Paper]](http://arxiv.org/pdf/2110.06831v2)[[Code]](https://github.com/decisionforce/EGPO.git)![](https://img.shields.io/github/stars/decisionforce/EGPO.svg?style=social&label=Star&maxAge=2592000)
    
- Batch policy learning under constraints(ICML, 2019)[[Paper]](http://arxiv.org/pdf/1903.08738v1)[[Code]](https://github.com/gwthomas/force.git)![](https://img.shields.io/github/stars/gwthomas/force.svg?style=social&label=Star&maxAge=2592000)
    

### sim2real, domain adaptation
- Segmented encoding for sim2real of rl-based end-to-end autonomous driving(IV, 2022)
- Learning interactive driving policies via datadriven simulation(ICRA, 2022)
- Simulation-based reinforcement learning for real-world autonomous driving(ICRA, 2020)[[Paper]](https://arxiv.org/abs/1911.12905)[[Code]](https://github.com/deepsense-ai/carla-birdeye-view.git)![](https://img.shields.io/github/stars/deepsense-ai/carla-birdeye-view.svg?style=social&label=Star&maxAge=2592000)
    
- Virtual to real reinforcement learning for autonomous driving(BMVC, 2017)[[Paper]](http://arxiv.org/pdf/1704.03952v4)[[Code]](https://github.com/SullyChen/Autopilot-TensorFlow.git)![](https://img.shields.io/github/stars/SullyChen/Autopilot-TensorFlow.svg?style=social&label=Star&maxAge=2592000)
    
- Domain adaptation in reinforcement learning via latent unified state representation(AAAI, 2021)[[Paper]](http://arxiv.org/pdf/2102.05714v2)[[Code]](https://github.com/KarlXing/LUSR.git)![](https://img.shields.io/github/stars/KarlXing/LUSR.svg?style=social&label=Star&maxAge=2592000)
    

### IL pre-training + RL
- Safe driving via expert guided policy optimization(CoRL, 2022)[[Paper]](http://arxiv.org/pdf/2110.06831v2)[[Code]](https://github.com/decisionforce/EGPO.git)![](https://img.shields.io/github/stars/decisionforce/EGPO.svg?style=social&label=Star&maxAge=2592000)
    
- Human visual attention prediction boosts learning & performance of autonomous driving agents(ICRA, 2020)[[Paper]](https://arxiv.org/abs/1909.05003)
    
- End-to-end learning of driving models with surround-view cameras and route planners(ECCV, 2018)[[Paper]](https://arxiv.org/abs/1803.10158)
    
- End-to-end multimodal sensors fusion system for urban automated driving(NeurIPSW, 2018)

### RL teacher + IL student
- Trajectoryguided control prediction for end-to-end autonomous driving: A simple yet strong baseline(NeurIPS, 2022)
- End-to-end urban driving by imitating a reinforcement learning coach(ICCV, 2021)[[Paper]](https://arxiv.org/abs/2108.08265)[[Code]](https://github.com/zhejz/carla-roach.git)![](https://img.shields.io/github/stars/zhejz/carla-roach.svg?style=social&label=Star&maxAge=2592000)
    
- Learning to drive from a world on rails(ICCV, 2021)[[Paper]](http://arxiv.org/pdf/2105.00636v3)[[Code]](https://github.com/dotchen/WorldOnRails.git)![](https://img.shields.io/github/stars/dotchen/WorldOnRails.svg?style=social&label=Star&maxAge=2592000)
    

### Camera Only
- Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d(ECCV, 2020)[[Paper]](https://arxiv.org/abs/2008.05711)[[Code]](https://github.com/nv-tlabs/lift-splat-shoot.git)![](https://img.shields.io/github/stars/nv-tlabs/lift-splat-shoot.svg?style=social&label=Star&maxAge=2592000)
- ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning(ECCV, 2022)

- Planning-oriented Autonomous Driving(CVPR, 2023)

- Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving(CVPR, 2023)


### LiDAR
- Differentiable raycasting for self-supervised occupancy forecasting(ECCV, 2022)[[Paper]](https://arxiv.org/abs/2210.01917)[[Code]](https://github.com/tarashakhurana/emergent-occ-forecasting.git)![](https://img.shields.io/github/stars/tarashakhurana/emergent-occ-forecasting.svg?style=social&label=Star&maxAge=2592000)
    
- Dsdnet: Deep structured self-driving network(ECCV, 2020)[[Paper]](https://arxiv.org/abs/2008.06041)
    
- End-to-end interpretable neural motion planner(CVPR, 2019)[[Paper]](https://arxiv.org/abs/2101.06679)[[Code]](https://github.com/tarashakhurana/emergent-occ-forecasting.git)![](https://img.shields.io/github/stars/tarashakhurana/emergent-occ-forecasting.svg?style=social&label=Star&maxAge=2592000)
    
- Lookout: Diverse multi-future prediction and planning for self-driving(ICCV, 2021)[[Paper]](https://arxiv.org/abs/2101.06547)
    
- Safe local motion planning with self-supervised freespace forecasting(CVPR, 2021)
- Mp3: A unified model to map, perceive, predict and plan(CVPR, 2021)[[Paper]](https://arxiv.org/abs/2101.06806)
    
- Learning interpretable end-to-end vision-based motion planning for autonomous driving with optical flow distillation[[Paper]](https://arxiv.org/abs/2104.12861)
    
- Perceive, predict, and plan: Safe motion planning through interpretable semantic representations(ECCV, 2020)[[Paper]](https://arxiv.org/abs/2008.05930)
    
   
- Dynamic conditional imitation learning for autonomous driving(T-ITS, 2022)[[Paper]](http://arxiv.org/pdf/2102.00675v1)[[Code]](https://github.com/heshameraqi/Dynamic-CIL-Driving.git)![](https://img.shields.io/github/stars/heshameraqi/Dynamic-CIL-Driving.svg?style=social&label=Star&maxAge=2592000)
    




## Benchmark and Dataset
### Closed-loop
- [CARLA](https://carla.org/)
- [CARLA Leaderboard](https://leaderboard.carla.org/leaderboard/)
- [nuPlan](https://www.nuscenes.org/nuplan)

### Open-loop
- [nuScenes](https://www.nuscenes.org/nuscenes)
- [Argoverse](https://www.argoverse.org/av2.html)
- [Waymo Open Dataset](https://waymo.com/open/)

## Contributing
Thank you for all your contributions. Please make sure to read the [contributing guide](./CONTRIBUTING.md) before you make a pull request.


## License
New Outlooks on End-to-end Autonomous Driving is released under the [MIT license](./LICENSE).


## Citation
```BibTeX
@article{chen2023e2esurvey,
  title={End-to-End Autonomous Driving: Challenges and Frontiers},
  author={Chen, Li and Wu, Penghao and Chitta, Kashyap and Jaeger, Bernhard and Geiger, Andreas and Li, Hongyang},
  journal={arXiv preprint arXiv:},
  year={2023}
}
```
